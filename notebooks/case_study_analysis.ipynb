{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case Study Analysis: Transformers vs Capsules\n",
    "\n",
    "This notebook provides initial analysis of the collected data for the Transformers vs Capsules case study.\n",
    "\n",
    "## Overview\n",
    "\n",
    "This analysis supports the Conveyance Framework research by examining differential adoption patterns between:\n",
    "\n",
    "- **Transformers**: \"Attention Is All You Need\" (arXiv:1706.03762)\n",
    "- **Capsule Networks**: \"Dynamic Routing Between Capsules\" (arXiv:1710.09829)\n",
    "\n",
    "## Data Sources\n",
    "\n",
    "1. Citation timelines from Semantic Scholar\n",
    "2. GitHub repository implementations\n",
    "3. Paper content and embeddings\n",
    "4. Boundary objects (documentation, code)\n",
    "5. Semantic embeddings in ArangoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data paths\n",
    "data_dir = Path(\"../data/case_study\")\n",
    "\n",
    "# Load citation data\n",
    "with open(data_dir / \"citations\" / \"transformers_citations.json\") as f:\n",
    "    transformers_citations = json.load(f)\n",
    "\n",
    "with open(data_dir / \"citations\" / \"capsules_citations.json\") as f:\n",
    "    capsules_citations = json.load(f)\n",
    "\n",
    "print(f\"Transformers total citations: {transformers_citations['total_citations']}\")\n",
    "print(f\"Capsules total citations: {capsules_citations['total_citations']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load repository data\n",
    "with open(data_dir / \"implementations\" / \"transformers_repos.json\") as f:\n",
    "    transformers_repos = json.load(f)\n",
    "\n",
    "with open(data_dir / \"implementations\" / \"capsules_repos.json\") as f:\n",
    "    capsules_repos = json.load(f)\n",
    "\n",
    "print(f\"Transformers repositories: {transformers_repos['total_repositories']}\")\n",
    "print(f\"Capsules repositories: {capsules_repos['total_repositories']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Citation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to pandas DataFrames\n",
    "def monthly_to_df(monthly_data, paper_name):\n",
    "    df = pd.DataFrame(monthly_data)\n",
    "    df['date'] = pd.to_datetime(df[['year', 'month']].assign(day=1))\n",
    "    df['paper'] = paper_name\n",
    "    df['cumulative'] = df['count'].cumsum()\n",
    "    return df\n",
    "\n",
    "trans_df = monthly_to_df(transformers_citations['monthly_citations'], 'Transformers')\n",
    "caps_df = monthly_to_df(capsules_citations['monthly_citations'], 'Capsules')\n",
    "\n",
    "citation_df = pd.concat([trans_df, caps_df])\n",
    "\n",
    "# Display summary statistics\n",
    "print(\"\\nCitation Statistics:\")\n",
    "print(citation_df.groupby('paper')['count'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot citation growth\n",
    "fig = px.line(\n",
    "    citation_df,\n",
    "    x='date',\n",
    "    y='cumulative',\n",
    "    color='paper',\n",
    "    title='Cumulative Citation Growth Comparison',\n",
    "    labels={'cumulative': 'Cumulative Citations', 'date': 'Date'},\n",
    "    template='plotly_white'\n",
    ")\n",
    "fig.update_layout(hovermode='x unified')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate growth rates\n",
    "def calculate_growth_rate(df):\n",
    "    # Monthly growth rate\n",
    "    df['growth_rate'] = df['count'].pct_change() * 100\n",
    "    return df\n",
    "\n",
    "trans_df = calculate_growth_rate(trans_df)\n",
    "caps_df = calculate_growth_rate(caps_df)\n",
    "\n",
    "print(\"\\nAverage monthly growth rate (first year):\")\n",
    "print(f\"Transformers: {trans_df.head(12)['growth_rate'].mean():.2f}%\")\n",
    "print(f\"Capsules: {caps_df.head(12)['growth_rate'].mean():.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Repository Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repository type distribution\n",
    "print(\"\\nRepository Type Distribution:\")\n",
    "print(\"\\nTransformers:\")\n",
    "for repo_type, count in transformers_repos['type_counts'].items():\n",
    "    print(f\"  {repo_type}: {count}\")\n",
    "\n",
    "print(\"\\nCapsules:\")\n",
    "for repo_type, count in capsules_repos['type_counts'].items():\n",
    "    print(f\"  {repo_type}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate total stars\n",
    "trans_stars = sum(repo['stars'] for repo in transformers_repos['repositories'])\n",
    "caps_stars = sum(repo['stars'] for repo in capsules_repos['repositories'])\n",
    "\n",
    "print(f\"\\nTotal GitHub stars:\")\n",
    "print(f\"Transformers: {trans_stars:,}\")\n",
    "print(f\"Capsules: {caps_stars:,}\")\n",
    "print(f\"Ratio: {trans_stars / caps_stars:.2f}x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time to first implementation\n",
    "def days_to_first_repo(paper_config, repos_data):\n",
    "    pub_date = datetime.fromisoformat(paper_config['published_date'])\n",
    "    \n",
    "    first_repo = min(\n",
    "        repos_data['repositories'],\n",
    "        key=lambda r: datetime.fromisoformat(r['created_at'].replace('Z', ''))\n",
    "    )\n",
    "    \n",
    "    first_date = datetime.fromisoformat(first_repo['created_at'].replace('Z', ''))\n",
    "    days = (first_date - pub_date).days\n",
    "    \n",
    "    return days, first_repo\n",
    "\n",
    "# This would require config data - placeholder for now\n",
    "print(\"\\nTime to first implementation analysis:\")\n",
    "print(\"(Requires publication date data)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Boundary Objects Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load boundary objects\n",
    "try:\n",
    "    with open(data_dir / \"boundary_objects\" / \"transformers\" / \"boundary_objects.json\") as f:\n",
    "        trans_bo = json.load(f)\n",
    "    \n",
    "    with open(data_dir / \"boundary_objects\" / \"capsules\" / \"boundary_objects.json\") as f:\n",
    "        caps_bo = json.load(f)\n",
    "    \n",
    "    print(\"Boundary Objects Count:\")\n",
    "    print(f\"Transformers: {trans_bo['total_objects']}\")\n",
    "    print(f\"Capsules: {caps_bo['total_objects']}\")\n",
    "    \n",
    "    # Type distribution\n",
    "    print(\"\\nTransformers boundary object types:\")\n",
    "    from collections import Counter\n",
    "    trans_types = Counter(obj['type'] for obj in trans_bo['boundary_objects'])\n",
    "    for obj_type, count in trans_types.items():\n",
    "        print(f\"  {obj_type}: {count}\")\n",
    "    \n",
    "    print(\"\\nCapsules boundary object types:\")\n",
    "    caps_types = Counter(obj['type'] for obj in caps_bo['boundary_objects'])\n",
    "    for obj_type, count in caps_types.items():\n",
    "        print(f\"  {obj_type}: {count}\")\n",
    "        \n",
    "except FileNotFoundError:\n",
    "    print(\"Boundary objects not yet collected. Run script 04.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Semantic Analysis (From Embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load embedding coordinates\n",
    "try:\n",
    "    with open(data_dir / \"visualizations\" / \"embedding_coordinates.json\") as f:\n",
    "        coords_data = json.load(f)\n",
    "    \n",
    "    coords = np.array(coords_data['coordinates'])\n",
    "    labels = coords_data['labels']\n",
    "    metadata = coords_data['metadata']\n",
    "    \n",
    "    print(f\"Loaded {len(coords)} embedding coordinates\")\n",
    "    \n",
    "    # Analyze semantic clustering\n",
    "    from sklearn.metrics import silhouette_score\n",
    "    \n",
    "    # Create cluster labels by paper\n",
    "    cluster_labels = [meta['paper'] for meta in metadata]\n",
    "    unique_papers = list(set(cluster_labels))\n",
    "    cluster_ids = [unique_papers.index(label) for label in cluster_labels]\n",
    "    \n",
    "    silhouette = silhouette_score(coords, cluster_ids)\n",
    "    print(f\"\\nSilhouette score (paper clustering): {silhouette:.3f}\")\n",
    "    print(\"(Higher = better separation between papers)\")\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(\"Embeddings not yet generated. Run scripts 05 and 06.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Conveyance Framework Metrics (Placeholder)\n",
    "\n",
    "This section will calculate Conveyance Framework metrics:\n",
    "\n",
    "- **W** (Writeability): Quality of paper writing\n",
    "- **R** (Readability): Ease of understanding\n",
    "- **C_ext** (External Context): Availability of boundary objects\n",
    "- **P_ij** (Prior Knowledge): Community readiness\n",
    "- **T** (Trust): Author/institution credibility\n",
    "\n",
    "These require manual annotation and additional data sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholder for C_ext calculation\n",
    "def calculate_c_ext(boundary_objects, implementations):\n",
    "    \"\"\"\n",
    "    Calculate external context score.\n",
    "    \n",
    "    Factors:\n",
    "    - Number of boundary objects\n",
    "    - Quality of documentation\n",
    "    - Official code availability\n",
    "    - Tutorial availability\n",
    "    \"\"\"\n",
    "    # Placeholder implementation\n",
    "    score = 0.0\n",
    "    \n",
    "    # Official code bonus\n",
    "    has_official = any(repo['type'] == 'official' for repo in implementations['repositories'])\n",
    "    if has_official:\n",
    "        score += 0.3\n",
    "    \n",
    "    # Boundary objects\n",
    "    bo_count = boundary_objects['total_objects']\n",
    "    score += min(bo_count / 20, 0.4)  # Max 0.4 for BO count\n",
    "    \n",
    "    # Tutorial availability\n",
    "    tutorial_count = sum(1 for repo in implementations['repositories'] if repo['type'] == 'tutorial')\n",
    "    score += min(tutorial_count / 10, 0.3)  # Max 0.3 for tutorials\n",
    "    \n",
    "    return min(score, 1.0)\n",
    "\n",
    "# Calculate for both papers (if data available)\n",
    "print(\"\\nC_ext scores (External Context):\")\n",
    "print(\"(Placeholder calculation - needs refinement)\")\n",
    "\n",
    "try:\n",
    "    trans_c_ext = calculate_c_ext(trans_bo, transformers_repos)\n",
    "    caps_c_ext = calculate_c_ext(caps_bo, capsules_repos)\n",
    "    \n",
    "    print(f\"Transformers: {trans_c_ext:.3f}\")\n",
    "    print(f\"Capsules: {caps_c_ext:.3f}\")\n",
    "    print(f\"Ratio: {trans_c_ext / caps_c_ext:.2f}x\")\n",
    "except:\n",
    "    print(\"Data not available yet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Summary Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate summary report\n",
    "print(\"=\"*60)\n",
    "print(\"CASE STUDY SUMMARY: Transformers vs Capsules\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\n1. CITATION IMPACT\")\n",
    "print(f\"   Transformers: {transformers_citations['total_citations']:,} citations\")\n",
    "print(f\"   Capsules: {capsules_citations['total_citations']:,} citations\")\n",
    "print(f\"   Ratio: {transformers_citations['total_citations'] / capsules_citations['total_citations']:.2f}x\")\n",
    "\n",
    "print(\"\\n2. IMPLEMENTATION ADOPTION\")\n",
    "print(f\"   Transformers: {transformers_repos['total_repositories']} repositories\")\n",
    "print(f\"   Capsules: {capsules_repos['total_repositories']} repositories\")\n",
    "print(f\"   Ratio: {transformers_repos['total_repositories'] / capsules_repos['total_repositories']:.2f}x\")\n",
    "\n",
    "print(\"\\n3. COMMUNITY ENGAGEMENT\")\n",
    "print(f\"   Transformers: {trans_stars:,} total stars\")\n",
    "print(f\"   Capsules: {caps_stars:,} total stars\")\n",
    "print(f\"   Ratio: {trans_stars / caps_stars:.2f}x\")\n",
    "\n",
    "try:\n",
    "    print(\"\\n4. BOUNDARY OBJECTS\")\n",
    "    print(f\"   Transformers: {trans_bo['total_objects']} objects\")\n",
    "    print(f\"   Capsules: {caps_bo['total_objects']} objects\")\n",
    "    print(f\"   Ratio: {trans_bo['total_objects'] / caps_bo['total_objects']:.2f}x\")\n",
    "except:\n",
    "    print(\"\\n4. BOUNDARY OBJECTS\")\n",
    "    print(\"   Not yet collected\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Next steps:\")\n",
    "print(\"1. Manual annotation for W, R, P_ij, T metrics\")\n",
    "print(\"2. Statistical significance testing\")\n",
    "print(\"3. Qualitative analysis of adoption patterns\")\n",
    "print(\"4. Conveyance model validation\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Export Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export summary data for further analysis\n",
    "summary = {\n",
    "    'transformers': {\n",
    "        'citations': transformers_citations['total_citations'],\n",
    "        'repositories': transformers_repos['total_repositories'],\n",
    "        'stars': trans_stars,\n",
    "        'type_distribution': transformers_repos['type_counts']\n",
    "    },\n",
    "    'capsules': {\n",
    "        'citations': capsules_citations['total_citations'],\n",
    "        'repositories': capsules_repos['total_repositories'],\n",
    "        'stars': caps_stars,\n",
    "        'type_distribution': capsules_repos['type_counts']\n",
    "    },\n",
    "    'analysis_date': datetime.now().isoformat()\n",
    "}\n",
    "\n",
    "with open(data_dir / 'summary_report.json', 'w') as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "\n",
    "print(\"Summary exported to data/case_study/summary_report.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
